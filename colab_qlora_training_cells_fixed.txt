# QLoRA å¾®è°ƒ Deepseek-7B - Colab è®­ç»ƒå•å…ƒæ ¼ï¼ˆä¿®å¤ç‰ˆï¼‰

## ğŸ”§ ä¿®å¤è¯´æ˜
- ä¿®å¤ bitsandbytes ç‰ˆæœ¬ä¸å…¼å®¹é—®é¢˜ï¼ˆCUDA 12.6ï¼‰
- ä¿®å¤ transformers ç‰ˆæœ¬å†²çª
- ä¼˜åŒ–æ•°æ®åº“æ–‡ä»¶å¤„ç†é€»è¾‘
- æ·»åŠ æ›´è¯¦ç»†çš„é”™è¯¯å¤„ç†

---

## å•å…ƒæ ¼ 1ï¼šç¯å¢ƒå‡†å¤‡ä¸æŒ‚è½½ Driveï¼ˆä¿®å¤ç‰ˆï¼‰

```python
# 1.1 æ£€æŸ¥ GPU
!nvidia-smi

# 1.2 æŒ‚è½½ Google Drive
from google.colab import drive
drive.mount('/content/drive')

# 1.3 åˆ‡æ¢åˆ°é¡¹ç›®ç›®å½•
%cd /content/drive/MyDrive/Graduation_Project

# ğŸ”§ ä¿®å¤ï¼šå®‰è£…å…¼å®¹çš„ä¾èµ–ç‰ˆæœ¬
print("å®‰è£…ä¾èµ–ï¼ˆå…¼å®¹ CUDA 12.xï¼‰...")

# å…³é”®ä¿®å¤ï¼šä½¿ç”¨å…¼å®¹ CUDA 12.x çš„ bitsandbytes ç‰ˆæœ¬
!pip install -q transformers==4.46.0
!pip install -q peft==0.13.0
!pip install -q bitsandbytes==0.44.1  # å…¼å®¹ CUDA 12.x
!pip install -q accelerate==1.0.0
!pip install -q datasets==3.0.0
!pip install -q trl==0.11.0

print("\nâœ“ ç¯å¢ƒå‡†å¤‡å®Œæˆ")
print("âœ“ ä¾èµ–ç‰ˆæœ¬ï¼š")
!pip show bitsandbytes transformers peft | grep "Name:\|Version:"
```

---

## å•å…ƒæ ¼ 2ï¼šå…‹éš†/æ›´æ–°ä»£ç ä»“åº“

```python
import os

# æ£€æŸ¥æ˜¯å¦å·²å…‹éš†
if not os.path.exists('/content/Graduation_Project'):
    print("å…‹éš†ä»£ç ä»“åº“...")
    !git clone https://github.com/Caria-Tarnished/Graduation_Project.git /content/Graduation_Project
else:
    print("æ›´æ–°ä»£ç ä»“åº“...")
    %cd /content/Graduation_Project
    !git pull

%cd /content/Graduation_Project

print("\nâœ“ ä»£ç ä»“åº“å‡†å¤‡å®Œæˆ")
```

---

## å•å…ƒæ ¼ 3ï¼šç”ŸæˆæŒ‡ä»¤é›†æ•°æ®ï¼ˆä¿®å¤ç‰ˆï¼‰

```python
# 3.1 æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨
import os
from pathlib import Path

db_path = Path('finance_analysis.db')
drive_db_path = Path('/content/drive/MyDrive/Graduation_Project/finance_analysis.db')

# ğŸ”§ ä¿®å¤ï¼šæ”¹è¿›æ•°æ®åº“æ£€æŸ¥é€»è¾‘
if not db_path.exists():
    if drive_db_path.exists():
        print("âš  æ•°æ®åº“ä¸å­˜åœ¨ï¼Œä» Drive å¤åˆ¶...")
        !cp /content/drive/MyDrive/Graduation_Project/finance_analysis.db .
        print("âœ“ æ•°æ®åº“å¤åˆ¶å®Œæˆ")
    else:
        print("âš  è­¦å‘Šï¼šDrive ä¸­ä¹Ÿæ²¡æœ‰æ•°æ®åº“æ–‡ä»¶")
        print("âš  å°†ä½¿ç”¨æ¨¡æ¿æ•°æ®ç”ŸæˆæŒ‡ä»¤é›†ï¼ˆä¸åŒ…å«çœŸå®æ–°é—»ï¼‰")
else:
    print("âœ“ æ•°æ®åº“å·²å­˜åœ¨")

# 3.2 ç”ŸæˆæŒ‡ä»¤é›†
print("\nç”ŸæˆæŒ‡ä»¤é›†æ•°æ®...")
!python scripts/qlora/build_instruction_dataset.py \
    --db finance_analysis.db \
    --output data/qlora/instructions.jsonl \
    --num_samples 300

# 3.3 æŸ¥çœ‹ç”Ÿæˆçš„æ•°æ®
print("\næŸ¥çœ‹å‰ 3 æ¡æŒ‡ä»¤ï¼š")
import json

with open('data/qlora/instructions.jsonl', 'r', encoding='utf-8') as f:
    for i, line in enumerate(f):
        if i >= 3:
            break
        data = json.loads(line)
        print(f"\n--- æŒ‡ä»¤ {i+1} ---")
        print(f"Instruction: {data['instruction']}")
        print(f"Input: {data['input'][:80]}...")
        print(f"Output: {data['output'][:100]}...")

print("\nâœ“ æŒ‡ä»¤é›†ç”Ÿæˆå®Œæˆ")
```

---

## å•å…ƒæ ¼ 4ï¼šå¼€å§‹ QLoRA å¾®è°ƒï¼ˆä¸»è®­ç»ƒ - ä¿®å¤ç‰ˆï¼‰

```python
# 4.1 è®¾ç½®è¾“å‡ºç›®å½•
output_dir = '/content/drive/MyDrive/Graduation_Project/qlora_output'

# 4.2 å¼€å§‹è®­ç»ƒ
print("="*60)
print("å¼€å§‹ QLoRA å¾®è°ƒ...")
print("="*60)
print("é¢„è®¡è®­ç»ƒæ—¶é—´ï¼š2-4 å°æ—¶")
print("è¯·ä¿æŒ Colab é¡µé¢æ‰“å¼€ï¼Œé¿å…æ–­å¼€è¿æ¥")
print("="*60)

# ğŸ”§ ä¿®å¤ï¼šæ·»åŠ é”™è¯¯å¤„ç†
try:
    !python scripts/qlora/train_qlora.py \
        --model_name deepseek-ai/deepseek-llm-7b-chat \
        --data_path data/qlora/instructions.jsonl \
        --output_dir {output_dir} \
        --num_epochs 3 \
        --batch_size 4 \
        --learning_rate 2e-4 \
        --max_length 512 \
        --lora_r 8 \
        --lora_alpha 16 \
        --lora_dropout 0.05
    
    print("\nâœ“ è®­ç»ƒå®Œæˆ")
except Exception as e:
    print(f"\nâœ— è®­ç»ƒå¤±è´¥: {e}")
    print("\nè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶åé¦ˆ")
```

---

## å•å…ƒæ ¼ 5ï¼šéªŒè¯è®­ç»ƒç»“æœ

```python
import os
from pathlib import Path
import json

output_dir = Path('/content/drive/MyDrive/Graduation_Project/qlora_output')

print("="*60)
print("è®­ç»ƒç»“æœéªŒè¯")
print("="*60)

# 5.1 æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
print("\n1. æ£€æŸ¥è¾“å‡ºæ–‡ä»¶ï¼š")
required_files = [
    'adapter_model.bin',
    'adapter_config.json',
    'training_info.json'
]

for file in required_files:
    file_path = output_dir / file
    if file_path.exists():
        size_mb = file_path.stat().st_size / (1024 * 1024)
        print(f"   âœ“ {file} ({size_mb:.2f} MB)")
    else:
        print(f"   âœ— {file} (ä¸å­˜åœ¨)")

# 5.2 è¯»å–è®­ç»ƒä¿¡æ¯
print("\n2. è®­ç»ƒä¿¡æ¯ï¼š")
info_path = output_dir / 'training_info.json'
if info_path.exists():
    with open(info_path, 'r', encoding='utf-8') as f:
        info = json.load(f)
    
    print(f"   æ¨¡å‹: {info['model_name']}")
    print(f"   è®­ç»ƒè½®æ•°: {info['num_epochs']}")
    print(f"   æ ·æœ¬æ•°é‡: {info['num_samples']}")
    print(f"   å­¦ä¹ ç‡: {info['learning_rate']}")
    print(f"   LoRA rank: {info['lora_r']}")
    print(f"   å®Œæˆæ—¶é—´: {info['timestamp']}")
else:
    print("   âš  è®­ç»ƒä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨")

# 5.3 è®¡ç®—æ€»å¤§å°
print("\n3. è¾“å‡ºç›®å½•æ€»å¤§å°ï¼š")
if output_dir.exists():
    total_size = sum(f.stat().st_size for f in output_dir.rglob('*') if f.is_file())
    print(f"   {total_size / (1024 * 1024):.2f} MB")
else:
    print("   âš  è¾“å‡ºç›®å½•ä¸å­˜åœ¨")

print("\n" + "="*60)
print("âœ“ éªŒè¯å®Œæˆ")
print("="*60)
```

---

## å•å…ƒæ ¼ 6ï¼šæµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹ï¼ˆå¯é€‰ - ä¿®å¤ç‰ˆï¼‰

```python
# 6.1 åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
print("åŠ è½½å¾®è°ƒåçš„æ¨¡å‹...")

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

try:
    # åŠ è½½åŸºç¡€æ¨¡å‹
    base_model_name = "deepseek-ai/deepseek-llm-7b-chat"
    model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        load_in_4bit=True,
        torch_dtype=torch.float16,
        device_map='auto',
        trust_remote_code=True
    )

    # åŠ è½½ LoRA æƒé‡
    output_dir = '/content/drive/MyDrive/Graduation_Project/qlora_output'
    model = PeftModel.from_pretrained(model, output_dir)

    # åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)

    print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")

    # 6.2 æµ‹è¯•æ¨ç†
    print("\næµ‹è¯•æ¨ç†...")

    test_cases = [
        {
            "instruction": "åˆ†æä»¥ä¸‹è´¢ç»å¿«è®¯å¯¹å¸‚åœºçš„å½±å“",
            "input": "ç¾è”å‚¨å®£å¸ƒåŠ æ¯25ä¸ªåŸºç‚¹"
        },
        {
            "instruction": "è§£é‡Šä»€ä¹ˆæ˜¯é¢„æœŸå…‘ç°",
            "input": "å¸‚åœºå‰æœŸå·²ç»å¤§æ¶¨ï¼Œåˆ©å¥½æ¶ˆæ¯å‘å¸ƒååè€Œä¸‹è·Œï¼Œè¿™æ˜¯ä¸ºä»€ä¹ˆï¼Ÿ"
        }
    ]

    for i, test in enumerate(test_cases, 1):
        print(f"\n--- æµ‹è¯•æ¡ˆä¾‹ {i} ---")
        print(f"Instruction: {test['instruction']}")
        print(f"Input: {test['input']}")
        
        # æ„å»º prompt
        prompt = f"User: {test['instruction']}\n{test['input']}\n\nAssistant:"
        
        # Tokenize
        inputs = tokenizer(prompt, return_tensors='pt').to(model.device)
        
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.7,
                top_p=0.9,
                do_sample=True
            )
        
        # è§£ç 
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå– Assistant çš„å›å¤
        if "Assistant:" in response:
            response = response.split("Assistant:")[-1].strip()
        
        print(f"Output: {response}")

    print("\nâœ“ æµ‹è¯•å®Œæˆ")

except Exception as e:
    print(f"\nâœ— æµ‹è¯•å¤±è´¥: {e}")
    print("å¯èƒ½åŸå› ï¼š")
    print("1. LoRA æƒé‡æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆè®­ç»ƒæœªå®Œæˆï¼‰")
    print("2. æ¨¡å‹åŠ è½½å¤±è´¥ï¼ˆæ˜¾å­˜ä¸è¶³ï¼‰")
```

---

## å•å…ƒæ ¼ 7ï¼šä¸‹è½½æƒé‡æ–‡ä»¶åˆ°æœ¬åœ°ï¼ˆå¯é€‰ï¼‰

```python
# å¦‚æœéœ€è¦ä¸‹è½½åˆ°æœ¬åœ°ç”µè„‘ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç 

from google.colab import files
from pathlib import Path

# ä¸‹è½½ LoRA æƒé‡
output_dir = Path('/content/drive/MyDrive/Graduation_Project/qlora_output')

if output_dir.exists():
    print("å‡†å¤‡ä¸‹è½½æ–‡ä»¶...")
    print("æ³¨æ„ï¼šæ–‡ä»¶è¾ƒå¤§ï¼Œä¸‹è½½å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ")

    # æ‰“åŒ…ä¸º zip
    !cd {output_dir} && zip -r qlora_weights.zip adapter_model.bin adapter_config.json training_info.json

    # ä¸‹è½½
    files.download(f'{output_dir}/qlora_weights.zip')

    print("âœ“ ä¸‹è½½å®Œæˆ")
else:
    print("âœ— è¾“å‡ºç›®å½•ä¸å­˜åœ¨ï¼Œæ— æ³•ä¸‹è½½")
```

---

## ğŸ”§ å…³é”®ä¿®å¤è¯´æ˜

### ä¿®å¤ 1: bitsandbytes ç‰ˆæœ¬å…¼å®¹æ€§
**é—®é¢˜**ï¼šåŸç‰ˆæœ¬ `bitsandbytes==0.41.3` ä¸æ”¯æŒ CUDA 12.6
**è§£å†³**ï¼šå‡çº§åˆ° `bitsandbytes==0.44.1`ï¼ˆæ”¯æŒ CUDA 12.xï¼‰

### ä¿®å¤ 2: transformers ç‰ˆæœ¬å†²çª
**é—®é¢˜**ï¼š`transformers==4.36.0` ä¸ `sentence-transformers` å†²çª
**è§£å†³**ï¼šå‡çº§åˆ° `transformers==4.46.0`

### ä¿®å¤ 3: æ•°æ®åº“æ–‡ä»¶å¤„ç†
**é—®é¢˜**ï¼šæ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨å¯¼è‡´æŒ‡ä»¤é›†ç”Ÿæˆå¤±è´¥
**è§£å†³**ï¼šæ·»åŠ é™çº§ç­–ç•¥ï¼Œä½¿ç”¨æ¨¡æ¿æ•°æ®ç”ŸæˆæŒ‡ä»¤é›†

### ä¿®å¤ 4: é”™è¯¯å¤„ç†
**é—®é¢˜**ï¼šè®­ç»ƒå¤±è´¥æ—¶æ²¡æœ‰æ˜ç¡®æç¤º
**è§£å†³**ï¼šæ·»åŠ  try-except é”™è¯¯æ•è·

---

## ğŸ“‹ ä½¿ç”¨è¯´æ˜

1. **åœ¨ Colab åˆ›å»ºæ–°ç¬”è®°æœ¬**
2. **é€‰æ‹© GPU è¿è¡Œæ—¶**ï¼ˆRuntime â†’ Change runtime type â†’ T4 GPUï¼‰
3. **æŒ‰é¡ºåºè¿è¡Œå•å…ƒæ ¼ 1-5**
4. **å•å…ƒæ ¼ 6-7 å¯é€‰**ï¼ˆç”¨äºæµ‹è¯•å’Œä¸‹è½½ï¼‰

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### å…³äºæ•°æ®åº“æ–‡ä»¶
- å¦‚æœ Drive ä¸­æ²¡æœ‰ `finance_analysis.db`ï¼Œè®­ç»ƒä»å¯ç»§ç»­
- æŒ‡ä»¤é›†ä¼šä½¿ç”¨æ¨¡æ¿æ•°æ®ï¼ˆ120 æ¡ï¼Œä¸åŒ…å«çœŸå®æ–°é—»ï¼‰
- æ¨¡æ¿æ•°æ®è¶³å¤Ÿç”¨äºæ¼”ç¤ºå’Œç­”è¾©

### å…³äºè®­ç»ƒæ—¶é—´
- 120 æ¡æ ·æœ¬ï¼šçº¦ 30-60 åˆ†é’Ÿ
- 300 æ¡æ ·æœ¬ï¼šçº¦ 2-4 å°æ—¶
- å»ºè®®å…ˆç”¨ 120 æ¡æµ‹è¯•ï¼Œç¡®è®¤æ— è¯¯åå†å¢åŠ æ ·æœ¬

### å…³äº Colab æ–­å¼€è¿æ¥
- å…è´¹ç‰ˆ Colab å¯èƒ½åœ¨ 12 å°æ—¶åæ–­å¼€
- å»ºè®®ä½¿ç”¨ Colab Proï¼ˆæ›´ç¨³å®šï¼‰
- æˆ–è€…åˆ†æ‰¹è®­ç»ƒï¼ˆå‡å°‘ epochsï¼‰

---

## ğŸ“ ç­”è¾©å‡†å¤‡

è®­ç»ƒå®Œæˆåï¼Œè¯·ä¿å­˜ä»¥ä¸‹ææ–™ï¼š

1. **è®­ç»ƒæ—¥å¿—æˆªå›¾**ï¼ˆå•å…ƒæ ¼ 4 çš„è¾“å‡ºï¼‰
2. **éªŒè¯ç»“æœæˆªå›¾**ï¼ˆå•å…ƒæ ¼ 5 çš„è¾“å‡ºï¼‰
3. **æµ‹è¯•ç»“æœæˆªå›¾**ï¼ˆå•å…ƒæ ¼ 6 çš„è¾“å‡ºï¼‰
4. **LoRA æƒé‡æ–‡ä»¶**ï¼ˆadapter_model.binï¼Œçº¦ 30-50 MBï¼‰

è¿™äº›ææ–™è¶³å¤Ÿè¯æ˜ä½ å®Œæˆäº† QLoRA å¾®è°ƒå·¥ä½œï¼

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼** ğŸš€
