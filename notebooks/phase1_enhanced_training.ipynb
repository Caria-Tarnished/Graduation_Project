{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 1: Enhanced BERT Training with Input Augmentation\n",
                "\n",
                "This notebook trains a BERT model on the enhanced dataset with market context prefixes.\n",
                "\n",
                "**Key improvements:**\n",
                "1. Input Augmentation: Add market state prefixes (e.g., `[Strong Rally]`, `[Sharp Decline]`)\n",
                "2. Class Weighting: Handle severe class imbalance with automatic weights\n",
                "3. Optimized hyperparameters: Lower LR, longer sequences, early stopping\n",
                "\n",
                "**Expected results:**\n",
                "- Macro F1: 0.35-0.45 (vs baseline 0.16)\n",
                "- Class 3/4/5 F1: > 0.10 (vs baseline 0.00)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Setup Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU availability\n",
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install/upgrade dependencies\n",
                "!pip install -U transformers datasets evaluate accelerate huggingface_hub -q"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Mount Google Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Clone Repository (First Time Only)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone repo (skip if already cloned)\n",
                "import os\n",
                "if not os.path.exists('Graduation_Project'):\n",
                "    !git clone https://github.com/Caria-Tarnished/Graduation_Project.git\n",
                "else:\n",
                "    print(\"Repository already exists, pulling latest changes...\")\n",
                "    !cd Graduation_Project && git pull\n",
                "\n",
                "%cd Graduation_Project"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Verify Enhanced Data in Drive\n",
                "\n",
                "**IMPORTANT**: Make sure you have uploaded these files to your Google Drive:\n",
                "- `/content/drive/MyDrive/Graduation_Project/data/processed/train_enhanced.csv`\n",
                "- `/content/drive/MyDrive/Graduation_Project/data/processed/val_enhanced.csv`\n",
                "- `/content/drive/MyDrive/Graduation_Project/data/processed/test_enhanced.csv`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if enhanced data exists\n",
                "import os\n",
                "\n",
                "DATA_DIR = '/content/drive/MyDrive/Graduation_Project/data/processed'\n",
                "files_to_check = ['train_enhanced.csv', 'val_enhanced.csv', 'test_enhanced.csv']\n",
                "\n",
                "print(\"Checking for enhanced data files...\")\n",
                "all_exist = True\n",
                "for filename in files_to_check:\n",
                "    filepath = os.path.join(DATA_DIR, filename)\n",
                "    exists = os.path.exists(filepath)\n",
                "    status = \"?\" if exists else \"?\"\n",
                "    print(f\"{status} {filepath}\")\n",
                "    if not exists:\n",
                "        all_exist = False\n",
                "\n",
                "if all_exist:\n",
                "    print(\"\\n? All enhanced data files found!\")\n",
                "else:\n",
                "    print(\"\\n?? Some files are missing. Please upload them to Google Drive first.\")\n",
                "    print(\"\\nLocal files location: E:\\\\Projects\\\\Graduation_Project\\\\data\\\\processed\\\\*_enhanced.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Preview Enhanced Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Load a sample\n",
                "train_path = '/content/drive/MyDrive/Graduation_Project/data/processed/train_enhanced.csv'\n",
                "df = pd.read_csv(train_path)\n",
                "\n",
                "print(f\"Training samples: {len(df)}\")\n",
                "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
                "print(f\"\\nLabel distribution:\")\n",
                "print(df['label_multi_cls'].value_counts().sort_index())\n",
                "\n",
                "print(f\"\\n{'='*80}\")\n",
                "print(\"Sample enhanced texts:\")\n",
                "print(f\"{'='*80}\")\n",
                "for i in range(3):\n",
                "    print(f\"\\n{i+1}. Label: {df.iloc[i]['label_multi_cls']}\")\n",
                "    print(f\"   Original: {df.iloc[i]['text'][:80]}...\")\n",
                "    print(f\"   Enhanced: {df.iloc[i]['text_enhanced'][:120]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Train Enhanced Model\n",
                "\n",
                "**Training configuration:**\n",
                "- Model: `hfl/chinese-roberta-wwm-ext`\n",
                "- Epochs: 5\n",
                "- Learning rate: 1e-5\n",
                "- Max length: 384 (increased for longer context)\n",
                "- Batch size: 16 (effective 32 with gradient accumulation)\n",
                "- Class weighting: Auto (handles imbalance)\n",
                "- Early stopping: 3 patience"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set output directory\n",
                "OUTPUT_DIR = '/content/drive/MyDrive/Graduation_Project/experiments/bert_enhanced_v1'\n",
                "\n",
                "# Training command\n",
                "!python scripts/modeling/bert_finetune_cls.py \\\n",
                "  --train_csv /content/drive/MyDrive/Graduation_Project/data/processed/train_enhanced.csv \\\n",
                "  --val_csv /content/drive/MyDrive/Graduation_Project/data/processed/val_enhanced.csv \\\n",
                "  --test_csv /content/drive/MyDrive/Graduation_Project/data/processed/test_enhanced.csv \\\n",
                "  --output_dir {OUTPUT_DIR} \\\n",
                "  --label_col label_multi_cls \\\n",
                "  --model_name hfl/chinese-roberta-wwm-ext \\\n",
                "  --class_weight auto \\\n",
                "  --epochs 5 \\\n",
                "  --lr 1e-5 \\\n",
                "  --max_length 384 \\\n",
                "  --train_bs 16 \\\n",
                "  --eval_bs 32 \\\n",
                "  --gradient_accumulation_steps 2 \\\n",
                "  --warmup_ratio 0.06 \\\n",
                "  --weight_decay 0.01 \\\n",
                "  --eval_steps 100 \\\n",
                "  --save_steps 100 \\\n",
                "  --early_stopping_patience 3"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Check Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "OUTPUT_DIR = '/content/drive/MyDrive/Graduation_Project/experiments/bert_enhanced_v1'\n",
                "\n",
                "# Load metrics\n",
                "with open(f'{OUTPUT_DIR}/metrics_val.json', 'r') as f:\n",
                "    val_metrics = json.load(f)\n",
                "\n",
                "with open(f'{OUTPUT_DIR}/metrics_test.json', 'r') as f:\n",
                "    test_metrics = json.load(f)\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"Validation Metrics\")\n",
                "print(\"=\"*80)\n",
                "print(f\"Accuracy: {val_metrics['eval_accuracy']:.4f}\")\n",
                "print(f\"Macro F1: {val_metrics['eval_macro_f1']:.4f}\")\n",
                "print(f\"Loss: {val_metrics['eval_loss']:.4f}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"Test Metrics\")\n",
                "print(\"=\"*80)\n",
                "print(f\"Accuracy: {test_metrics['eval_accuracy']:.4f}\")\n",
                "print(f\"Macro F1: {test_metrics['eval_macro_f1']:.4f}\")\n",
                "print(f\"Loss: {test_metrics['eval_loss']:.4f}\")\n",
                "\n",
                "# Compare with baseline\n",
                "baseline_f1 = 0.163\n",
                "improvement = (test_metrics['eval_macro_f1'] - baseline_f1) / baseline_f1 * 100\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"Comparison with Baseline\")\n",
                "print(\"=\"*80)\n",
                "print(f\"Baseline Macro F1: {baseline_f1:.4f}\")\n",
                "print(f\"Enhanced Macro F1: {test_metrics['eval_macro_f1']:.4f}\")\n",
                "print(f\"Improvement: {improvement:+.1f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show classification report\n",
                "with open(f'{OUTPUT_DIR}/report_test.txt', 'r') as f:\n",
                "    report = f.read()\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"Classification Report (Test Set)\")\n",
                "print(\"=\"*80)\n",
                "print(report)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Analyze Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load predictions\n",
                "pred_df = pd.read_csv(f'{OUTPUT_DIR}/pred_test.csv')\n",
                "\n",
                "print(\"Prediction distribution:\")\n",
                "print(pred_df['pred'].value_counts().sort_index())\n",
                "\n",
                "print(\"\\nConfusion matrix (simplified):\")\n",
                "from sklearn.metrics import confusion_matrix\n",
                "import numpy as np\n",
                "\n",
                "cm = confusion_matrix(pred_df['label'], pred_df['pred'])\n",
                "print(cm)\n",
                "\n",
                "# Check if Class 3/4/5 are being predicted\n",
                "rare_classes = [3, 4, 5]\n",
                "for cls in rare_classes:\n",
                "    count = (pred_df['pred'] == cls).sum()\n",
                "    print(f\"\\nClass {cls} predictions: {count}\")\n",
                "    if count > 0:\n",
                "        print(\"? Model is predicting this class!\")\n",
                "    else:\n",
                "        print(\"?? Model never predicts this class\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Download Results (Optional)\n",
                "\n",
                "If you want to download the results to your local machine:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a zip file of results\n",
                "import shutil\n",
                "\n",
                "OUTPUT_DIR = '/content/drive/MyDrive/Graduation_Project/experiments/bert_enhanced_v1'\n",
                "zip_path = '/content/bert_enhanced_v1_results'\n",
                "\n",
                "# Copy only small files (metrics, reports, predictions)\n",
                "!mkdir -p {zip_path}\n",
                "!cp {OUTPUT_DIR}/metrics_*.json {zip_path}/\n",
                "!cp {OUTPUT_DIR}/report_*.txt {zip_path}/\n",
                "!cp {OUTPUT_DIR}/pred_*.csv {zip_path}/\n",
                "!cp {OUTPUT_DIR}/eval_results.json {zip_path}/ 2>/dev/null || true\n",
                "\n",
                "# Create zip\n",
                "shutil.make_archive('/content/bert_enhanced_v1_results', 'zip', zip_path)\n",
                "\n",
                "print(\"? Results packaged!\")\n",
                "print(\"Download: /content/bert_enhanced_v1_results.zip\")\n",
                "\n",
                "# Download using Colab's file browser or:\n",
                "from google.colab import files\n",
                "files.download('/content/bert_enhanced_v1_results.zip')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "Based on the results:\n",
                "\n",
                "### If Macro F1 > 0.35 ?\n",
                "- Proceed to Phase 2: Try financial pre-trained model (`mengzi-bert-base-fin`)\n",
                "- Implement Focal Loss for better minority class handling\n",
                "\n",
                "### If Macro F1 < 0.30 ??\n",
                "- Revisit data quality (check label correctness)\n",
                "- Simplify to 3-class problem (bearish/neutral/bullish only)\n",
                "- Try different labeling thresholds\n",
                "\n",
                "### If Class 3/4/5 still have F1=0 ??\n",
                "- Try manual class weights: `{0:0.5, 1:2.0, 2:2.0, 3:20.0, 4:20.0, 5:3.0}`\n",
                "- Consider merging Class 3/4 into single \"priced-in\" class\n",
                "- Generate synthetic samples using LLM"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
