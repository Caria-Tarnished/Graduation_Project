# ============================================================
# 单元格 1: 环境准备
# ============================================================
from google.colab import drive
drive.mount('/content/drive')

!pip install -U transformers datasets evaluate accelerate -q

import os
if not os.path.exists('/content/Graduation_Project'):
    !git clone https://github.com/Caria-Tarnished/Graduation_Project.git
else:
    !cd /content/Graduation_Project && git pull

%cd /content/Graduation_Project


# ============================================================
# 单元格 2: 验证/生成增强数据
# ============================================================
import os
import pandas as pd

files = {
    'train': '/content/Graduation_Project/data/processed/train_enhanced.csv',
    'val': '/content/Graduation_Project/data/processed/val_enhanced.csv',
    'test': '/content/Graduation_Project/data/processed/test_enhanced.csv'
}

print("数据文件检查:")
all_exist = True
for name, path in files.items():
    if os.path.exists(path):
        df = pd.read_csv(path, encoding="utf-8")
        print(f"? {name}: {len(df)} 样本")
    else:
        print(f"? {name}: 文件不存在")
        all_exist = False

if not all_exist:
    print("\n?? 数据文件缺失，正在生成...")
    !python scripts/modeling/build_enhanced_dataset.py \
      --input_dir /content/Graduation_Project/data/processed \
      --output_dir /content/Graduation_Project/data/processed
    print("\n? 数据生成完成！")


# ============================================================
# 单元格 3: 预览增强数据（可选）
# ============================================================
import pandas as pd

train_df = pd.read_csv('/content/Graduation_Project/data/processed/train_enhanced.csv', encoding="utf-8")

print(f"训练集样本数: {len(train_df)}")
print(f"\n标签分布:")
print(train_df['label_multi_cls'].value_counts().sort_index())

print(f"\n{'='*80}")
print("增强文本示例:")
print(f"{'='*80}")
for i in range(3):
    print(f"\n{i+1}. Label: {train_df.iloc[i]['label_multi_cls']}")
    print(f"   原文: {train_df.iloc[i]['text'][:60]}...")
    print(f"   增强: {train_df.iloc[i]['text_enhanced'][:100]}...")


# ============================================================
# 单元格 4: Phase 1 训练（关键！）
# 注意：确保先 git push 最新的训练脚本，然后在单元格1中 git pull
# ============================================================
import os
import sys
import subprocess

import pandas as pd
import torch

use_gpu = bool(torch.cuda.is_available())

train_csv = "/content/Graduation_Project/data/processed/train_enhanced.csv"
val_csv = "/content/Graduation_Project/data/processed/val_enhanced.csv"
test_csv = "/content/Graduation_Project/data/processed/test_enhanced.csv"

output_dir = "/content/drive/MyDrive/Graduation_Project/experiments/bert_enhanced_v1"
if not use_gpu:
    output_dir = "/content/drive/MyDrive/Graduation_Project/experiments/bert_enhanced_v1_cpu_smoke"
    os.makedirs(os.path.dirname(output_dir), exist_ok=True)

    train_df = pd.read_csv(train_csv, encoding="utf-8")
    val_df = pd.read_csv(val_csv, encoding="utf-8")
    test_df = pd.read_csv(test_csv, encoding="utf-8")

    train_csv = "/content/Graduation_Project/data/processed/train_enhanced_cpu_smoke.csv"
    val_csv = "/content/Graduation_Project/data/processed/val_enhanced_cpu_smoke.csv"
    test_csv = "/content/Graduation_Project/data/processed/test_enhanced_cpu_smoke.csv"

    train_df.head(800).to_csv(train_csv, index=False, encoding="utf-8")
    val_df.head(200).to_csv(val_csv, index=False, encoding="utf-8")
    test_df.head(400).to_csv(test_csv, index=False, encoding="utf-8")

cmd = [
    sys.executable,
    "scripts/modeling/bert_finetune_cls.py",
    "--train_csv",
    train_csv,
    "--val_csv",
    val_csv,
    "--test_csv",
    test_csv,
    "--output_dir",
    output_dir,
    "--label_col",
    "label_multi_cls",
    "--text_col",
    "text_enhanced",
    "--model_name",
    "hfl/chinese-roberta-wwm-ext",
    "--class_weight",
    "auto",
    "--lr",
    "1e-5",
    "--warmup_ratio",
    "0.06",
    "--weight_decay",
    "0.01",
    "--early_stopping_patience",
    "0",
]

if use_gpu:
    cmd += [
        "--epochs",
        "5",
        "--max_length",
        "384",
        "--train_bs",
        "16",
        "--eval_bs",
        "32",
        "--gradient_accumulation_steps",
        "2",
        "--eval_steps",
        "100",
        "--save_steps",
        "100",
    ]
else:
    cmd += [
        "--epochs",
        "1",
        "--max_length",
        "256",
        "--train_bs",
        "4",
        "--eval_bs",
        "8",
        "--gradient_accumulation_steps",
        "1",
        "--eval_steps",
        "200",
        "--save_steps",
        "200",
    ]

print("use_gpu =", use_gpu)
print("output_dir =", output_dir)
print("CMD:\n", " ".join(cmd))
proc = subprocess.Popen(
    cmd,
    stdout=subprocess.PIPE,
    stderr=subprocess.STDOUT,
    text=True,
    bufsize=1,
)

tail_lines = []
assert proc.stdout is not None
for line in proc.stdout:
    print(line, end="")
    tail_lines.append(line)
    if len(tail_lines) > 200:
        tail_lines = tail_lines[-200:]

ret = proc.wait()
if ret != 0:
    print("\n" + "=" * 80)
    print("TRAIN PROCESS FAILED. LAST 200 LINES:")
    print("=" * 80)
    print("".join(tail_lines))
    raise RuntimeError(f"Training process exited with code {ret}")


# ============================================================
# 单元格 5: 查看结果
# ============================================================
import json
import pandas as pd

OUTPUT_DIR = output_dir if "output_dir" in globals() else "/content/drive/MyDrive/Graduation_Project/experiments/bert_enhanced_v1"

# 测试集指标
with open(f'{OUTPUT_DIR}/metrics_test.json', 'r') as f:
    test_metrics = json.load(f)

print("="*80)
print("Phase 1 训练结果")
print("="*80)
print(f"Test Accuracy: {test_metrics['eval_accuracy']:.4f}")
print(f"Test Macro F1: {test_metrics['eval_macro_f1']:.4f}")

# 与基线对比
baseline_f1 = 0.163
improvement = (test_metrics['eval_macro_f1'] - baseline_f1) / baseline_f1 * 100
print(f"\n与基线对比:")
print(f"  基线: {baseline_f1:.4f}")
print(f"  增强: {test_metrics['eval_macro_f1']:.4f}")
print(f"  提升: {improvement:+.1f}%")

# 分类报告
print("\n" + "="*80)
print("分类报告")
print("="*80)
with open(f'{OUTPUT_DIR}/report_test.txt', 'r') as f:
    print(f.read())

# 预测分析
pred_df = pd.read_csv(f'{OUTPUT_DIR}/pred_test.csv')
print("\n预测分布:")
print(pred_df['pred'].value_counts().sort_index())

# 稀有类别
rare_classes = [3, 4, 5]
print("\n稀有类别预测:")
for cls in rare_classes:
    pred_count = (pred_df['pred'] == cls).sum()
    true_count = (pred_df['label'] == cls).sum()
    print(f"  Class {cls}: 真实={true_count}, 预测={pred_count}")
    if pred_count > 0:
        print(f"    ? 模型开始预测这个类别了！")
