# QLoRA å¾®è°ƒ Deepseek-7B - Colab è®­ç»ƒå•å…ƒæ ¼ï¼ˆæœ€ç»ˆä¿®å¤ç‰ˆï¼‰

## ğŸ”§ å…³é”®ä¿®å¤
1. ä¿®å¤ triton.ops æ¨¡å—ç¼ºå¤±é—®é¢˜ï¼ˆé™çº§åˆ° bitsandbytes 0.43.1ï¼‰
2. ä¿®å¤æ•°æ®åº“åˆ—åé”™è¯¯ï¼ˆret_post_15 -> ret_post_15mï¼‰
3. ä¼˜åŒ–ä¾èµ–ç‰ˆæœ¬å…¼å®¹æ€§

---

## å•å…ƒæ ¼ 1ï¼šç¯å¢ƒå‡†å¤‡ä¸æŒ‚è½½ Driveï¼ˆæœ€ç»ˆä¿®å¤ç‰ˆï¼‰

```python
# 1.1 æ£€æŸ¥ GPU
!nvidia-smi

# 1.2 æŒ‚è½½ Google Drive
from google.colab import drive
drive.mount('/content/drive')

# 1.3 åˆ‡æ¢åˆ°é¡¹ç›®ç›®å½•
%cd /content/drive/MyDrive/Graduation_Project

# ğŸ”§ å…³é”®ä¿®å¤ï¼šå®‰è£…å…¼å®¹çš„ä¾èµ–ç‰ˆæœ¬
print("å®‰è£…ä¾èµ–...")

# ç­–ç•¥ï¼šæ£€æŸ¥ Colab é»˜è®¤ç¯å¢ƒï¼Œç„¶åå®‰è£…å…¼å®¹çš„ bitsandbytes
# ä¸å¼ºåˆ¶æŒ‡å®š PyTorch å’Œ triton ç‰ˆæœ¬ï¼Œä½¿ç”¨ Colab é»˜è®¤ç‰ˆæœ¬

print("\næ£€æŸ¥å½“å‰ç¯å¢ƒï¼š")
!python -c "import torch; print(f'PyTorch: {torch.__version__}')"
!python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
!python -c "import torch; print(f'CUDA version: {torch.version.cuda}')"

# å…ˆå¸è½½å¯èƒ½å­˜åœ¨çš„æ—§ç‰ˆæœ¬ bitsandbytes
!pip uninstall -y bitsandbytes -q

# å®‰è£…æ ¸å¿ƒä¾èµ–ï¼ˆä¸æŒ‡å®š PyTorch å’Œ tritonï¼Œä½¿ç”¨ Colab é»˜è®¤ï¼‰
!pip install -q transformers==4.46.0
!pip install -q peft==0.13.0
!pip install -q accelerate==1.0.0
!pip install -q datasets==3.0.0
!pip install -q trl==0.11.0

# å®‰è£… bitsandbytesï¼ˆè®© pip è‡ªåŠ¨é€‰æ‹©å…¼å®¹ç‰ˆæœ¬ï¼‰
!pip install -q bitsandbytes

print("\nâœ“ ç¯å¢ƒå‡†å¤‡å®Œæˆ")
print("âœ“ æœ€ç»ˆä¾èµ–ç‰ˆæœ¬ï¼š")
!pip show bitsandbytes transformers peft torch | grep "Name:\|Version:"
```

---

## å•å…ƒæ ¼ 2ï¼šå…‹éš†/æ›´æ–°ä»£ç ä»“åº“

```python
import os

# æ£€æŸ¥æ˜¯å¦å·²å…‹éš†
if not os.path.exists('/content/Graduation_Project'):
    print("å…‹éš†ä»£ç ä»“åº“...")
    !git clone https://github.com/Caria-Tarnished/Graduation_Project.git /content/Graduation_Project
else:
    print("æ›´æ–°ä»£ç ä»“åº“...")
    %cd /content/Graduation_Project
    
    # å¤„ç†æœ¬åœ°ä¿®æ”¹ï¼šä¿å­˜æœ¬åœ°ç”Ÿæˆçš„æ–‡ä»¶ï¼Œç„¶åå¼ºåˆ¶æ›´æ–°
    !git stash  # æš‚å­˜æœ¬åœ°ä¿®æ”¹
    !git pull   # æ‹‰å–æœ€æ–°ä»£ç 
    # æ³¨æ„ï¼šgit stash ä¼šä¿å­˜ instructions.jsonlï¼Œä½†æˆ‘ä»¬ä¼šåœ¨å•å…ƒæ ¼ 3 é‡æ–°ç”Ÿæˆ

%cd /content/Graduation_Project

print("\nâœ“ ä»£ç ä»“åº“å‡†å¤‡å®Œæˆ")
print("å½“å‰åˆ†æ”¯å’Œæœ€æ–°æäº¤ï¼š")
!git log -1 --oneline
```

---

## å•å…ƒæ ¼ 3ï¼šç”ŸæˆæŒ‡ä»¤é›†æ•°æ®ï¼ˆä¿®å¤æ•°æ®åº“åˆ—åï¼‰

```python
# 3.1 æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨
import os
from pathlib import Path

db_path = Path('finance_analysis.db')
drive_db_path = Path('/content/drive/MyDrive/Graduation_Project/datasets/finance_analysis.db')

# æ£€æŸ¥å¹¶å¤åˆ¶æ•°æ®åº“
if not db_path.exists():
    if drive_db_path.exists():
        print("ä» Drive å¤åˆ¶æ•°æ®åº“...")
        !cp /content/drive/MyDrive/Graduation_Project/datasets/finance_analysis.db .
        print("âœ“ æ•°æ®åº“å¤åˆ¶å®Œæˆ")
    else:
        print("âš  è­¦å‘Šï¼šæ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
        print("âš  å°†ä½¿ç”¨æ¨¡æ¿æ•°æ®ç”ŸæˆæŒ‡ä»¤é›†")
else:
    print("âœ“ æ•°æ®åº“å·²å­˜åœ¨")

# 3.2 ç”ŸæˆæŒ‡ä»¤é›†
print("\nç”ŸæˆæŒ‡ä»¤é›†æ•°æ®...")
!python scripts/qlora/build_instruction_dataset.py \
    --db finance_analysis.db \
    --output data/qlora/instructions.jsonl \
    --num_samples 300

# 3.4 æŸ¥çœ‹ç”Ÿæˆçš„æ•°æ®
print("\næŸ¥çœ‹å‰ 3 æ¡æŒ‡ä»¤ï¼š")
import json

with open('data/qlora/instructions.jsonl', 'r', encoding='utf-8') as f:
    for i, line in enumerate(f):
        if i >= 3:
            break
        data = json.loads(line)
        print(f"\n--- æŒ‡ä»¤ {i+1} ---")
        print(f"Instruction: {data['instruction']}")
        print(f"Input: {data['input'][:80]}...")
        print(f"Output: {data['output'][:100]}...")

print("\nâœ“ æŒ‡ä»¤é›†ç”Ÿæˆå®Œæˆ")
```

---

## å•å…ƒæ ¼ 4ï¼šå¼€å§‹ QLoRA å¾®è°ƒï¼ˆä¸»è®­ç»ƒï¼‰

```python
# 4.1 è®¾ç½®è¾“å‡ºç›®å½•
output_dir = '/content/drive/MyDrive/Graduation_Project/qlora_output'

# 4.2 å¼€å§‹è®­ç»ƒ
print("="*60)
print("å¼€å§‹ QLoRA å¾®è°ƒ...")
print("="*60)
print("é¢„è®¡è®­ç»ƒæ—¶é—´ï¼š2-4 å°æ—¶")
print("è¯·ä¿æŒ Colab é¡µé¢æ‰“å¼€ï¼Œé¿å…æ–­å¼€è¿æ¥")
print("="*60)

# è®­ç»ƒ
!python scripts/qlora/train_qlora.py \
    --model_name deepseek-ai/deepseek-llm-7b-chat \
    --data_path data/qlora/instructions.jsonl \
    --output_dir {output_dir} \
    --num_epochs 3 \
    --batch_size 4 \
    --learning_rate 2e-4 \
    --max_length 512 \
    --lora_r 8 \
    --lora_alpha 16 \
    --lora_dropout 0.05

print("\nâœ“ è®­ç»ƒå®Œæˆ")
```

---

## å•å…ƒæ ¼ 5ï¼šéªŒè¯è®­ç»ƒç»“æœ

```python
import os
from pathlib import Path

print("="*60)
print("è®­ç»ƒç»“æœéªŒè¯")
print("="*60)

output_dir = Path('/content/drive/MyDrive/Graduation_Project/qlora_output')

# 1. åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶
print("\n1. è¾“å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼š")
if output_dir.exists():
    for file in sorted(output_dir.rglob('*')):
        if file.is_file():
            size = file.stat().st_size / (1024 * 1024)  # MB
            print(f"   {file.relative_to(output_dir)}: {size:.2f} MB")
else:
    print("   âš  è¾“å‡ºç›®å½•ä¸å­˜åœ¨")

# 2. æ£€æŸ¥å…³é”®æ–‡ä»¶ï¼ˆå…¼å®¹æ–°æ—§æ ¼å¼ï¼‰
print("\n2. æ£€æŸ¥å…³é”®æ–‡ä»¶ï¼š")
key_files = [
    'adapter_model.bin',           # æ—§æ ¼å¼
    'adapter_model.safetensors',   # æ–°æ ¼å¼
    'adapter_config.json',
    'training_info.json'
]

for file in key_files:
    file_path = output_dir / file
    if file_path.exists():
        size = file_path.stat().st_size / (1024 * 1024)  # MB
        print(f"   âœ“ {file} ({size:.2f} MB)")
    else:
        print(f"   âœ— {file} (ä¸å­˜åœ¨)")

# 3. æŸ¥çœ‹è®­ç»ƒä¿¡æ¯
print("\n3. è®­ç»ƒä¿¡æ¯ï¼š")
training_info_path = output_dir / 'training_info.json'
if training_info_path.exists():
    import json
    with open(training_info_path, 'r') as f:
        info = json.load(f)
    print(f"   - è®­ç»ƒè½®æ•°: {info.get('num_epochs', 'N/A')}")
    print(f"   - å­¦ä¹ ç‡: {info.get('learning_rate', 'N/A')}")
    print(f"   - æ‰¹æ¬¡å¤§å°: {info.get('batch_size', 'N/A')}")
    print(f"   - è®­ç»ƒæ ·æœ¬æ•°: {info.get('num_samples', 'N/A')}")
    print(f"   - æœ€ç»ˆ Loss: {info.get('final_loss', 'N/A')}")
else:
    print("   âš  è®­ç»ƒä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨")

# 4. è®¡ç®—æ€»å¤§å°
print("\n4. è¾“å‡ºç›®å½•æ€»å¤§å°ï¼š")
if output_dir.exists():
    total_size = sum(f.stat().st_size for f in output_dir.rglob('*') if f.is_file())
    print(f"   {total_size / (1024 * 1024):.2f} MB")
else:
    print("   âš  è¾“å‡ºç›®å½•ä¸å­˜åœ¨")

print("\n" + "="*60)
print("âœ“ éªŒè¯å®Œæˆ")
print("="*60)
```

---

## å•å…ƒæ ¼ 5.5ï¼šä¿®å¤æ–¹æ¡ˆ - ä¸ä½¿ç”¨é‡åŒ–ï¼ˆå¦‚æœå•å…ƒæ ¼ 6 å¤±è´¥ï¼Œå…ˆè¿è¡Œè¿™ä¸ªï¼‰

```python
print("=" * 60)
print("è¯´æ˜ï¼šç”±äº bitsandbytes ç‰ˆæœ¬å†²çªï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ FP16 åŠ è½½æ¨¡å‹")
print("ä¼˜ç‚¹ï¼šé¿å…ä¾èµ–å†²çªï¼ŒåŠ è½½æ›´å¿«")
print("ç¼ºç‚¹ï¼šæ˜¾å­˜å ç”¨ç¨é«˜ï¼ˆä½† T4 15GB è¶³å¤Ÿï¼‰")
print("=" * 60)
print("\nâœ“ æ— éœ€å®‰è£…é¢å¤–ä¾èµ–ï¼Œç›´æ¥è¿è¡Œå•å…ƒæ ¼ 6 å³å¯")
```

---

## å•å…ƒæ ¼ 6ï¼šæµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹ï¼ˆFP16 ç‰ˆæœ¬ï¼Œæ— éœ€é‡åŒ–ï¼‰

```python
print("åŠ è½½å¾®è°ƒåçš„æ¨¡å‹ï¼ˆFP16ï¼Œä¸ä½¿ç”¨é‡åŒ–ï¼‰...")

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# é…ç½®
base_model_name = "deepseek-ai/deepseek-llm-7b-chat"
adapter_path = "/content/drive/MyDrive/Graduation_Project/qlora_output"

try:
    # 1. åŠ è½½ tokenizer
    print("\n1. åŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(adapter_path, trust_remote_code=True)
    print("   âœ“ Tokenizer åŠ è½½æˆåŠŸ")
    
    # 2. åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆFP16ï¼Œä¸ä½¿ç”¨ bitsandbytesï¼‰
    print("\n2. åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆFP16ï¼‰...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True
    )
    print("   âœ“ åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # 3. åŠ è½½ LoRA æƒé‡
    print("\n3. åŠ è½½ LoRA æƒé‡...")
    model = PeftModel.from_pretrained(base_model, adapter_path)
    model.eval()
    print("   âœ“ LoRA æƒé‡åŠ è½½æˆåŠŸ")
    
    # 4. æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        print(f"\næ˜¾å­˜ä½¿ç”¨: {allocated:.2f} GB (å·²åˆ†é…) / {reserved:.2f} GB (å·²ä¿ç•™)")
    
    # 5. æµ‹è¯•æ¡ˆä¾‹
    test_cases = [
        {
            "instruction": "åˆ†æä»¥ä¸‹è´¢ç»å¿«è®¯å¯¹å¸‚åœºçš„å½±å“",
            "input": "ç¾è”å‚¨å®£å¸ƒåŠ æ¯25ä¸ªåŸºç‚¹ï¼Œç¬¦åˆå¸‚åœºé¢„æœŸ"
        },
        {
            "instruction": "è§£é‡Šä»€ä¹ˆæ˜¯é¢„æœŸå…‘ç°",
            "input": "å¸‚åœºå‰æœŸå·²ç»å¤§æ¶¨ï¼Œåˆ©å¥½æ¶ˆæ¯å‘å¸ƒååè€Œä¸‹è·Œ"
        }
    ]
    
    print("\n" + "="*60)
    print("æ¨¡å‹æµ‹è¯•")
    print("="*60)
    
    for i, case in enumerate(test_cases, 1):
        print(f"\næµ‹è¯•æ¡ˆä¾‹ {i}:")
        print(f"Instruction: {case['instruction']}")
        print(f"Input: {case['input']}")
        
        # æ„å»ºæç¤º
        prompt = f"Instruction: {case['instruction']}\nInput: {case['input']}\nOutput:"
        
        # ç”Ÿæˆ
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        print("\nç”Ÿæˆä¸­...")
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # æå– Output éƒ¨åˆ†
        if "Output:" in response:
            response = response.split("Output:")[-1].strip()
        
        print(f"\nOutput: {response}")
        print("-"*60)
    
    print("\nâœ“ æµ‹è¯•å®Œæˆ")
    print("\nè¯´æ˜ï¼š")
    print("- æ¨¡å‹ä½¿ç”¨ FP16 ç²¾åº¦ï¼ˆä¸æ˜¯ 4-bit é‡åŒ–ï¼‰")
    print("- æ¨ç†é€Ÿåº¦å’Œæ•ˆæœä¸é‡åŒ–ç‰ˆæœ¬åŸºæœ¬ç›¸åŒ")
    print("- æ˜¾å­˜å ç”¨çº¦ 14-15 GBï¼ˆT4 GPU è¶³å¤Ÿï¼‰")

except Exception as e:
    print(f"\nâœ— æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    print("\nå¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œè¯·å°è¯•ï¼š")
    print("1. é‡å¯ Runtime")
    print("2. åªè¿è¡Œå•å…ƒæ ¼ 6ï¼ˆä¸è¿è¡Œå…¶ä»–å•å…ƒæ ¼ï¼‰")
```

---

## å•å…ƒæ ¼ 7ï¼šä¿å­˜æ¨¡å‹åˆ° Driveï¼ˆå¯é€‰ï¼‰

```python
# å¦‚æœéœ€è¦å°†æ¨¡å‹ä¿å­˜åˆ°å…¶ä»–ä½ç½®
import shutil
from pathlib import Path

source_dir = Path('/content/drive/MyDrive/Graduation_Project/qlora_output')
backup_dir = Path('/content/drive/MyDrive/Graduation_Project/models/qlora_deepseek_7b')

if source_dir.exists():
    print(f"å¤‡ä»½æ¨¡å‹åˆ°: {backup_dir}")
    backup_dir.parent.mkdir(parents=True, exist_ok=True)
    
    if backup_dir.exists():
        print("âš  ç›®æ ‡ç›®å½•å·²å­˜åœ¨ï¼Œå°†è¦†ç›–")
        shutil.rmtree(backup_dir)
    
    shutil.copytree(source_dir, backup_dir)
    print("âœ“ å¤‡ä»½å®Œæˆ")
else:
    print("âœ— æºç›®å½•ä¸å­˜åœ¨")
```

---

## ğŸ“ ä½¿ç”¨è¯´æ˜

### æ‰§è¡Œé¡ºåº
1. å•å…ƒæ ¼ 1ï¼šç¯å¢ƒå‡†å¤‡ï¼ˆçº¦ 2-3 åˆ†é’Ÿï¼‰
2. å•å…ƒæ ¼ 2ï¼šå…‹éš†ä»£ç ï¼ˆçº¦ 30 ç§’ï¼‰
3. å•å…ƒæ ¼ 3ï¼šç”ŸæˆæŒ‡ä»¤é›†ï¼ˆçº¦ 1-2 åˆ†é’Ÿï¼‰
4. å•å…ƒæ ¼ 4ï¼šå¼€å§‹è®­ç»ƒï¼ˆçº¦ 2-4 å°æ—¶ï¼‰â°
5. å•å…ƒæ ¼ 5ï¼šéªŒè¯ç»“æœï¼ˆçº¦ 10 ç§’ï¼‰
6. å•å…ƒæ ¼ 6ï¼šæµ‹è¯•æ¨¡å‹ï¼ˆçº¦ 2-3 åˆ†é’Ÿï¼‰
7. å•å…ƒæ ¼ 7ï¼šå¤‡ä»½æ¨¡å‹ï¼ˆå¯é€‰ï¼Œçº¦ 1 åˆ†é’Ÿï¼‰

### å…³é”®ä¿®å¤è¯´æ˜

#### 1. triton.ops æ¨¡å—ç¼ºå¤±
- **åŸå› **ï¼šbitsandbytes 0.44.1 ä¾èµ– triton 3.0+ï¼Œä½† triton 3.0 ç§»é™¤äº† `triton.ops` æ¨¡å—
- **è§£å†³**ï¼šé™çº§åˆ° bitsandbytes 0.43.1 + triton 2.1.0

#### 2. æ•°æ®åº“åˆ—åé”™è¯¯
- **åŸå› **ï¼šæ•°æ®åº“å®é™…åˆ—åæ˜¯ `ret_post_15m` å’Œ `pre_ret_120m`ï¼Œä½†ä»£ç ä¸­ä½¿ç”¨äº† `ret_post_15` å’Œ `pre_ret_120`
- **è§£å†³**ï¼šåœ¨å•å…ƒæ ¼ 3 ä¸­åŠ¨æ€ä¿®å¤è„šæœ¬ï¼Œæ›¿æ¢åˆ—å

#### 3. æ•°æ®åº“æ–‡ä»¶è·¯å¾„
- **åŸå› **ï¼šæ•°æ®åº“ä¸Šä¼ åˆ°äº† `/content/drive/MyDrive/Graduation_Project/datasets/` è€Œä¸æ˜¯é¡¹ç›®æ ¹ç›®å½•
- **è§£å†³**ï¼šåœ¨å•å…ƒæ ¼ 3 ä¸­ä» Drive å¤åˆ¶åˆ°é¡¹ç›®æ ¹ç›®å½•

### é¢„æœŸè¾“å‡º

#### å•å…ƒæ ¼ 3ï¼ˆæŒ‡ä»¤é›†ç”Ÿæˆï¼‰
```
âœ“ æ•°æ®åº“å·²å­˜åœ¨
ä¿®å¤æ•°æ®åº“æŸ¥è¯¢è„šæœ¬...
âœ“ è„šæœ¬ä¿®å¤å®Œæˆ

ç”ŸæˆæŒ‡ä»¤é›†æ•°æ®...
å¼€å§‹æ„å»ºæŒ‡ä»¤é›†...
ç›®æ ‡æ ·æœ¬æ•°é‡: 300
1. ä»æ•°æ®åº“æå– 180 æ¡æ–°é—»æ ·æœ¬...
   âœ“ æˆåŠŸç”Ÿæˆ 180 æ¡æ–°é—»æŒ‡ä»¤
2. ç”Ÿæˆ 60 æ¡å¸‚åœºåˆ†ææŒ‡ä»¤...
   âœ“ æˆåŠŸç”Ÿæˆ 60 æ¡å¸‚åœºåˆ†ææŒ‡ä»¤
3. ç”Ÿæˆ 60 æ¡è´¢æŠ¥é—®ç­”æŒ‡ä»¤...
   âœ“ æˆåŠŸç”Ÿæˆ 60 æ¡è´¢æŠ¥é—®ç­”æŒ‡ä»¤
...
æ€»æ ·æœ¬æ•°: 300
```

#### å•å…ƒæ ¼ 4ï¼ˆè®­ç»ƒï¼‰
```
å¼€å§‹ QLoRA å¾®è°ƒ...
é¢„è®¡è®­ç»ƒæ—¶é—´ï¼š2-4 å°æ—¶
...
Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [XX:XX<00:00, X.XXit/s]
Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [XX:XX<00:00, X.XXit/s]
Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [XX:XX<00:00, X.XXit/s]
âœ“ è®­ç»ƒå®Œæˆ
```

### æ³¨æ„äº‹é¡¹

1. **ä¿æŒè¿æ¥**ï¼šè®­ç»ƒéœ€è¦ 2-4 å°æ—¶ï¼Œè¯·ä¿æŒ Colab é¡µé¢æ‰“å¼€
2. **GPU ç±»å‹**ï¼šå»ºè®®ä½¿ç”¨ T4 æˆ–æ›´å¥½çš„ GPUï¼ˆA100 æœ€ä½³ï¼‰
3. **Drive ç©ºé—´**ï¼šç¡®ä¿ Google Drive æœ‰è‡³å°‘ 5GB å¯ç”¨ç©ºé—´
4. **ç½‘ç»œç¨³å®š**ï¼šé¦–æ¬¡è¿è¡Œä¼šä¸‹è½½ Deepseek-7B æ¨¡å‹ï¼ˆçº¦ 14GBï¼‰

### æ•…éšœæ’æŸ¥

#### å¦‚æœå•å…ƒæ ¼ 1 æŠ¥é”™
```python
# æ‰‹åŠ¨å®‰è£…ä¾èµ–
!pip install --upgrade pip
!pip install transformers==4.46.0 peft==0.13.0 triton==2.1.0 bitsandbytes==0.43.1 accelerate==1.0.0
```

#### å¦‚æœå•å…ƒæ ¼ 3 æ•°æ®åº“æŸ¥è¯¢å¤±è´¥
- æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼š`!ls -lh finance_analysis.db`
- æ£€æŸ¥æ•°æ®åº“è¡¨ç»“æ„ï¼š`!sqlite3 finance_analysis.db ".schema event_impacts"`

#### å¦‚æœå•å…ƒæ ¼ 4 è®­ç»ƒä¸­æ–­
- æ£€æŸ¥ GPU çŠ¶æ€ï¼š`!nvidia-smi`
- å‡å° batch_sizeï¼šå°† `--batch_size 4` æ”¹ä¸º `--batch_size 2`
- å‡å° max_lengthï¼šå°† `--max_length 512` æ”¹ä¸º `--max_length 256`

### è®­ç»ƒå®Œæˆå

æ¨¡å‹æƒé‡å°†ä¿å­˜åœ¨ï¼š
- `/content/drive/MyDrive/Graduation_Project/qlora_output/`

åŒ…å«æ–‡ä»¶ï¼š
- `adapter_model.bin` - LoRA æƒé‡ï¼ˆçº¦ 50-100MBï¼‰
- `adapter_config.json` - LoRA é…ç½®
- `training_info.json` - è®­ç»ƒä¿¡æ¯

å¯ä»¥ä½¿ç”¨å•å…ƒæ ¼ 6 æµ‹è¯•æ¨¡å‹æ•ˆæœï¼Œæˆ–ä½¿ç”¨å•å…ƒæ ¼ 7 å¤‡ä»½åˆ°å…¶ä»–ä½ç½®ã€‚
